# %% [markdown]
# # ICD-10 Code Prediction from Symptoms using ClinicalBERT
#
# This notebook demonstrates how to fine-tune a ClinicalBERT model (specifically `emilyalsentzer/Bio_ClinicalBERT`)
# for multi-label classification to predict ICD-10 codes based on textual descriptions of patient symptoms.
#
# **IMPORTANT:** This notebook uses placeholder data. You need to replace the dummy data
# section with code to load your actual dataset containing symptom texts and their associated ICD-10 codes.

# %% [markdown]
# ## 1. Setup: Install and Import Libraries

# %%
# !pip install transformers torch pandas scikit-learn accelerate -q # Uncomment and run this line if libraries are not installed

# %%
import torch
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import f1_score, roc_auc_score, accuracy_score
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    EvalPrediction
)
import warnings
warnings.filterwarnings("ignore") # Optional: Hide warnings

# %% [markdown]
# ## 2. Configuration & Parameters

# %%
# --- Configuration ---
MODEL_NAME = "emilyalsentzer/Bio_ClinicalBERT" # Recommended ClinicalBERT model
MAX_LENGTH = 256  # Max sequence length for tokenization
BATCH_SIZE = 8   # Adjust based on your GPU memory
EPOCHS = 3       # Number of training epochs (start with a small number)
LEARNING_RATE = 2e-5 # Learning rate for the optimizer
OUTPUT_DIR = "./clinical_bert_icd10_model" # Directory to save the trained model
LOGGING_DIR = "./logs" # Directory for TensorBoard logs

# Determine device
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")

# %% [markdown]
# ## 3. Load and Prepare Data
#
# **---> REPLACE THIS SECTION WITH YOUR ACTUAL DATA LOADING <---**
#
# You need:
# 1. `texts`: A list of strings, where each string is a patient's symptom description.
# 2. `labels`: A list of lists, where each inner list contains the ICD-10 codes (as strings) for the corresponding text.

# %%
# --- Placeholder Data ---
# Example symptom descriptions
texts = [
    "Patient reports severe headache, sensitivity to light, and nausea.",
    "Experiencing persistent cough, fever, and shortness of breath for 3 days.",
    "Complains of sharp abdominal pain in the lower right quadrant, loss of appetite.",
    "Symptoms include runny nose, sore throat, and mild fever.",
    "Sudden onset of chest pain radiating to left arm, sweating.",
    "Feeling dizzy and lightheaded, especially upon standing.",
    "Chronic lower back pain, worse with movement.",
    "Rash appeared on arms, itchy and red.",
    "Difficulty sleeping, feeling anxious and restless.",
    "Frequent urination and excessive thirst noted."
]

# Example corresponding ICD-10 codes (as lists of strings)
# These are illustrative codes and may not be clinically accurate pairings!
labels = [
    ["G43.9", "R11.2"],         # Migraine, Nausea
    ["R05", "R50.9", "R06.02"], # Cough, Fever, Shortness of breath
    ["R10.11", "R63.0"],       # Right lower quadrant pain, Anorexia
    ["J00", "R07.0", "R50.9"],  # Common cold, Sore throat, Fever
    ["I21.9", "R20.0"],         # Acute MI (unspecified), Sweating (using related symptom code)
    ["R42", "R55"],             # Dizziness, Syncope/Collapse
    ["M54.5"],                 # Low back pain
    ["L30.9", "L29.9"],         # Dermatitis (unspecified), Pruritus (unspecified)
    ["F41.9", "R45.0"],         # Anxiety disorder (unspecified), Nervousness
    ["R35.0", "R63.1"]          # Polyuria, Polydipsia
]

print(f"Loaded {len(texts)} data points.")
# --- End Placeholder Data ---


# %% [markdown]
# ### 3.1 Preprocess Labels (Multi-Label Binarization)
# We need to convert the lists of ICD-10 code strings into a binary matrix format (multi-hot encoding).

# %%
# Identify all unique ICD-10 codes present in the dataset
# In a real scenario, you might load a predefined list of all possible target codes
mlb = MultiLabelBinarizer()
mlb.fit(labels) # Fit on the training labels to find all unique codes

num_unique_labels = len(mlb.classes_)
print(f"Found {num_unique_labels} unique ICD-10 codes.")
print("Classes (Codes):", mlb.classes_)

# Transform labels into the binary matrix format
encoded_labels = mlb.transform(labels).astype(np.float32) # Convert to float for loss calculation

# %% [markdown]
# ### 3.2 Split Data (Train/Validation)

# %%
# Split data into training and validation sets
train_texts, val_texts, train_labels, val_labels = train_test_split(
    texts, encoded_labels, test_size=0.2, random_state=42 # Use 20% for validation
)

print(f"Training samples: {len(train_texts)}")
print(f"Validation samples: {len(val_texts)}")

# %% [markdown]
# ### 3.3 Tokenize Text

# %%
# Load the tokenizer associated with the chosen ClinicalBERT model
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# %%
# Tokenize the training and validation texts
train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=MAX_LENGTH)
val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=MAX_LENGTH)

# %% [markdown]
# ### 3.4 Create PyTorch Dataset

# %%
class ICD10Dataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        # Return dictionary items expected by the Hugging Face model
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

# Create dataset objects
train_dataset = ICD10Dataset(train_encodings, train_labels)
val_dataset = ICD10Dataset(val_encodings, val_labels)

# %% [markdown]
# ## 4. Load and Configure Model

# %%
# Create id2label and label2id mappings (required by Trainer for metrics)
id2label = {i: label for i, label in enumerate(mlb.classes_)}
label2id = {label: i for i, label in enumerate(mlb.classes_)}

# %%
# Load the pre-trained ClinicalBERT model for sequence classification
# Configure it for multi-label classification
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=num_unique_labels,
    problem_type="multi_label_classification", # Critical for multi-label setup
    id2label=id2label,
    label2id=label2id
)

# Move model to the appropriate device (GPU or CPU)
model.to(DEVICE)
print("Model loaded and configured for multi-label classification.")

# %% [markdown]
# ## 5. Define Training Arguments and Metrics

# %%
# Define evaluation metrics function for multi-label classification
def multi_label_metrics(predictions, labels, threshold=0.5):
    # Apply sigmoid on predictions which are logits by default
    sigmoid = torch.nn.Sigmoid()
    probs = sigmoid(torch.Tensor(predictions))
    # Use threshold to turn probabilities into 0/1 predictions
    y_pred = np.zeros(probs.shape)
    y_pred[np.where(probs >= threshold)] = 1
    y_true = labels

    # Calculate metrics
    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')
    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro') # Can be 'micro' or 'macro'
    accuracy = accuracy_score(y_true, y_pred) # Exact match ratio

    metrics = {'f1': f1_micro_average,
               'roc_auc': roc_auc,
               'accuracy': accuracy}
    return metrics

# Wrapper function for Hugging Face Trainer
def compute_metrics(p: EvalPrediction):
    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
    result = multi_label_metrics(predictions=preds, labels=p.label_ids)
    return result

# %%
# Define training arguments
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,             # Directory to save model checkpoints
    num_train_epochs=EPOCHS,           # Total number of training epochs
    per_device_train_batch_size=BATCH_SIZE,  # Batch size per device during training
    per_device_eval_batch_size=BATCH_SIZE*2, # Batch size for evaluation
    learning_rate=LEARNING_RATE,       # Initial learning rate
    warmup_steps=100,                  # Number of warmup steps for learning rate scheduler
    weight_decay=0.01,                 # Strength of weight decay regularization
    logging_dir=LOGGING_DIR,           # Directory for storing logs
    logging_steps=50,                 # Log metrics every X updates steps
    evaluation_strategy="epoch",       # Evaluate model at the end of each epoch
    save_strategy="epoch",             # Save model checkpoint at the end of each epoch
    load_best_model_at_end=True,       # Load the best model checkpoint found during training
    metric_for_best_model="f1",        # Use F1 score to determine the best model
    greater_is_better=True,            # Higher F1 score is better
    report_to="tensorboard"            # Enable TensorBoard logging (optional)
    # fp16=True # Uncomment if using GPU with FP16 support for faster training
)

# %% [markdown]
# ## 6. Initialize and Run Trainer

# %%
# Initialize the Trainer
trainer = Trainer(
    model=model,                         # The instantiated Transformers model to be trained
    args=training_args,                  # Training arguments, defined above
    train_dataset=train_dataset,         # Training dataset
    eval_dataset=val_dataset,            # Evaluation dataset
    tokenizer=tokenizer,                 # Tokenizer for padding examples dynamically
    compute_metrics=compute_metrics      # Function to compute metrics during evaluation
)

# %%
# Start training
print("Starting training...")
trainer.train()

print("Training finished.")

# %% [markdown]
# ## 7. Evaluate the Best Model

# %%
print("Evaluating the best model on the validation set...")
eval_results = trainer.evaluate()

print("\nEvaluation Results:")
for key, value in eval_results.items():
    print(f"{key}: {value:.4f}")

# %% [markdown]
# ## 8. Save the Model and Tokenizer

# %%
print(f"Saving the best model and tokenizer to {OUTPUT_DIR}...")
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
# Also save the MultiLabelBinarizer for later use during inference
import joblib
joblib.dump(mlb, f'{OUTPUT_DIR}/multilabel_binarizer.joblib')

print("Model, tokenizer, and label binarizer saved.")

# %% [markdown]
# ## 9. Inference Example (Optional)
# How to use the trained model to predict codes for new symptom text.

# %%
# Load the saved model, tokenizer, and binarizer for inference
# You would do this in a separate script or notebook
model_fine_tuned = AutoModelForSequenceClassification.from_pretrained(OUTPUT_DIR)
tokenizer_fine_tuned = AutoTokenizer.from_pretrained(OUTPUT_DIR)
mlb_loaded = joblib.load(f'{OUTPUT_DIR}/multilabel_binarizer.joblib')

model_fine_tuned.to(DEVICE) # Move model to GPU if available
model_fine_tuned.eval()     # Set model to evaluation mode

# %%
# Example new symptom text
new_symptom = "Patient has high fever, chills, and muscle aches."

# Tokenize the new input
inputs = tokenizer_fine_tuned(new_symptom, return_tensors="pt", truncation=True, padding=True, max_length=MAX_LENGTH)
inputs = {k: v.to(DEVICE) for k, v in inputs.items()} # Move input tensors to the correct device

# Get predictions (logits)
with torch.no_grad():
    outputs = model_fine_tuned(**inputs)
    logits = outputs.logits

# Convert logits to probabilities and apply threshold
sigmoid = torch.nn.Sigmoid()
probs = sigmoid(logits.squeeze().cpu()) # Move probabilities to CPU
predictions = np.zeros(probs.shape)
predictions[np.where(probs >= 0.5)] = 1 # Use the same threshold as during training

# Decode the predicted binary vector back into ICD-10 codes
predicted_indices = np.where(predictions == 1)[0]
predicted_codes = mlb_loaded.classes_[predicted_indices]

print(f"\nSymptom Text: '{new_symptom}'")
print(f"Predicted ICD-10 Codes: {list(predicted_codes)}")